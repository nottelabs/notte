---
title: Reasoning Models
description: Choose the right LLM for your agent's reasoning and decision-making
---

Agents use large language models (LLMs) to understand pages, plan actions, and make decisions. Notte supports multiple model providers with different capabilities and pricing.

## Supported Models

### Gemini Models (Google)

**gemini/gemini-2.0-flash** (Recommended)

Fast and cost-effective model from Google with excellent reasoning capabilities.

```python
agent = client.Agent(
    session=session,
    reasoning_model="gemini/gemini-2.0-flash"
)
```

**Features:**
- ‚úÖ Fast inference
- ‚úÖ Vision support
- ‚úÖ Low cost
- ‚úÖ Good for most tasks

**Best for:** General-purpose automation, data extraction, navigation tasks

**Pricing:**
- $0.10 / 1M input tokens
- $0.40 / 1M output tokens

---

### Claude Models (Anthropic)

**anthropic/claude-3.5-sonnet**

Advanced reasoning model with excellent problem-solving and adaptability.

```python
agent = client.Agent(
    session=session,
    reasoning_model="anthropic/claude-3.5-sonnet"
)
```

**Features:**
- ‚úÖ Superior reasoning
- ‚úÖ Vision support
- ‚úÖ Handles complex tasks
- ‚ö†Ô∏è  Higher cost

**Best for:** Complex multi-step workflows, ambiguous tasks, error recovery

**Pricing:**
- $3.00 / 1M input tokens
- $15.00 / 1M output tokens

---

**anthropic/claude-3.5-haiku**

Faster, more affordable Claude model for simpler tasks.

```python
agent = client.Agent(
    session=session,
    reasoning_model="anthropic/claude-3.5-haiku"
)
```

**Features:**
- ‚úÖ Fast inference
- ‚úÖ Lower cost than Sonnet
- ‚úÖ Good reasoning
- ‚ö†Ô∏è  Less capable than Sonnet

**Best for:** Simple navigation, form filling, straightforward extraction

**Pricing:**
- $1.00 / 1M input tokens
- $5.00 / 1M output tokens

---

### OpenAI Models

**openai/gpt-4o**

OpenAI's flagship multimodal model with strong vision and reasoning.

```python
agent = client.Agent(
    session=session,
    reasoning_model="openai/gpt-4o"
)
```

**Features:**
- ‚úÖ Strong reasoning
- ‚úÖ Vision support
- ‚úÖ Reliable performance
- ‚ö†Ô∏è  Moderate cost

**Best for:** Tasks requiring consistent performance and good vision understanding

**Pricing:**
- $2.50 / 1M input tokens
- $10.00 / 1M output tokens

---

**openai/gpt-4o-mini**

Smaller, faster, and more affordable GPT-4 variant.

```python
agent = client.Agent(
    session=session,
    reasoning_model="openai/gpt-4o-mini"
)
```

**Features:**
- ‚úÖ Fast and affordable
- ‚úÖ Vision support
- ‚ö†Ô∏è  Less capable than GPT-4o

**Best for:** High-volume tasks where cost matters

**Pricing:**
- $0.15 / 1M input tokens
- $0.60 / 1M output tokens

## Model Comparison

| Model | Speed | Cost | Reasoning | Vision | Best Use Case |
|-------|-------|------|-----------|--------|---------------|
| **gemini-2.0-flash** | ‚ö°‚ö°‚ö° | üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | General purpose |
| **claude-3.5-sonnet** | ‚ö°‚ö° | üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Complex tasks |
| **claude-3.5-haiku** | ‚ö°‚ö°‚ö° | üí∞üí∞ | ‚≠ê‚≠ê‚≠ê | ‚úÖ | Simple tasks |
| **gpt-4o** | ‚ö°‚ö° | üí∞üí∞üí∞ | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚úÖ | Consistent performance |
| **gpt-4o-mini** | ‚ö°‚ö°‚ö° | üí∞ | ‚≠ê‚≠ê‚≠ê | ‚úÖ | High volume |

## Choosing a Model

### For Most Tasks: Gemini 2.0 Flash

Start with Gemini 2.0 Flash - it offers the best balance of speed, cost, and capability:

```python
agent = client.Agent(
    session=session,
    reasoning_model="gemini/gemini-2.0-flash",
    max_steps=15
)
```

**Advantages:**
- Fastest inference time
- Lowest cost per token
- Excellent for 80% of use cases
- Good vision understanding

### For Complex Tasks: Claude 3.5 Sonnet

Use Claude 3.5 Sonnet when tasks require advanced reasoning:

```python
agent = client.Agent(
    session=session,
    reasoning_model="anthropic/claude-3.5-sonnet",
    max_steps=30
)
```

**When to use:**
- Multi-step workflows with decision trees
- Ambiguous or poorly-defined tasks
- Error recovery and adaptation
- Complex data extraction

### For Simple Tasks: Claude 3.5 Haiku or GPT-4o Mini

Use cheaper models for straightforward automation:

```python
agent = client.Agent(
    session=session,
    reasoning_model="anthropic/claude-3.5-haiku",
    max_steps=10
)
```

**When to use:**
- Simple navigation (go to URL, click button)
- Form filling with known fields
- Basic data extraction
- High-volume tasks where cost matters

## Model Performance

### Token Usage

Agents typically use tokens for:
- **Input**: Page content, task description, conversation history
- **Output**: Reasoning, action decisions, extracted data

**Typical token usage per step:**
- Simple page: ~1,000-3,000 input tokens
- Complex page: ~5,000-10,000 input tokens
- Output per step: ~200-500 tokens

**Example calculation:**

An agent completing a 10-step task on medium-complexity pages:

```
Input tokens: 10 steps √ó 5,000 tokens = 50,000 tokens
Output tokens: 10 steps √ó 300 tokens = 3,000 tokens

Gemini 2.0 Flash cost:
- Input: 50,000 √ó $0.10 / 1M = $0.005
- Output: 3,000 √ó $0.40 / 1M = $0.001
- Total: $0.006

Claude 3.5 Sonnet cost:
- Input: 50,000 √ó $3.00 / 1M = $0.15
- Output: 3,000 √ó $15.00 / 1M = $0.045
- Total: $0.195
```

### Latency

Approximate time per agent step:

| Model | Latency per Step |
|-------|------------------|
| Gemini 2.0 Flash | 2-4 seconds |
| Claude 3.5 Haiku | 3-5 seconds |
| GPT-4o Mini | 3-5 seconds |
| GPT-4o | 4-7 seconds |
| Claude 3.5 Sonnet | 5-8 seconds |

## Vision Capabilities

All supported models include vision, but performance varies:

### Vision-Heavy Tasks

For sites with important visual elements:

```python
agent = client.Agent(
    session=session,
    reasoning_model="openai/gpt-4o",  # Strong vision
    use_vision=True
)
```

**Best models for vision:**
1. GPT-4o - Excellent image understanding
2. Claude 3.5 Sonnet - Strong visual reasoning
3. Gemini 2.0 Flash - Good vision, fastest

### Text-Only Tasks

Disable vision to reduce costs:

```python
agent = client.Agent(
    session=session,
    reasoning_model="gemini/gemini-2.0-flash",
    use_vision=False  # Cheaper without vision
)
```

## Model Requirements

### API Keys

Models require environment variables with API keys:

| Provider | Environment Variable | Required |
|----------|---------------------|----------|
| Google | `GOOGLE_API_KEY` | For Gemini models |
| Anthropic | `ANTHROPIC_API_KEY` | For Claude models |
| OpenAI | `OPENAI_API_KEY` | For GPT models |

Set your API keys:

```bash
export GOOGLE_API_KEY="your-key-here"
export ANTHROPIC_API_KEY="your-key-here"
export OPENAI_API_KEY="your-key-here"
```

Or use Notte's managed keys (no environment variables needed):

```python
# Notte handles API keys automatically
agent = client.Agent(
    session=session,
    reasoning_model="gemini/gemini-2.0-flash"
)
```

## Best Practices

### 1. Start with Gemini 2.0 Flash

Begin with the default and upgrade only if needed:

```python
# Start here
reasoning_model="gemini/gemini-2.0-flash"

# Upgrade if task is too complex
reasoning_model="anthropic/claude-3.5-sonnet"
```

### 2. Match Model to Task Complexity

| Task Complexity | Recommended Model |
|-----------------|-------------------|
| Simple (1-5 steps) | Gemini 2.0 Flash, Claude Haiku |
| Medium (5-15 steps) | Gemini 2.0 Flash, GPT-4o |
| Complex (15-30 steps) | Claude 3.5 Sonnet, GPT-4o |

### 3. Monitor Costs

Track token usage in production:

```python
result = agent.run(task="Complete task")

# Check token usage (if available in response)
print(f"Steps taken: {len(result.steps)}")
print(f"Success: {result.success}")
```

### 4. Test Multiple Models

For critical tasks, test different models:

```python
models = [
    "gemini/gemini-2.0-flash",
    "anthropic/claude-3.5-sonnet",
    "openai/gpt-4o"
]

for model in models:
    agent = client.Agent(
        session=session,
        reasoning_model=model
    )
    result = agent.run(task="Your task here")
    print(f"{model}: Success={result.success}")
```

## Next Steps

<CardGroup cols={2}>
  <Card title="Configuration" icon="sliders" href="/features/agents/configuration">
    Configure agents with optimal settings
  </Card>

  <Card title="Structured Output" icon="brackets-curly" href="/features/agents/structured-output">
    Get typed responses from any model
  </Card>

  <Card title="Pricing" icon="dollar-sign" href="/pricing">
    Understand Notte pricing
  </Card>

  <Card title="Best Practices" icon="lightbulb" href="/guides/reliability">
    Build reliable agents
  </Card>
</CardGroup>
