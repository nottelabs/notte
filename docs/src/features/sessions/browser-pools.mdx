---
title: Browser Pools
description: Manage multiple browser sessions with connection pooling
---

Browser pools allow you to manage multiple concurrent browser sessions efficiently, enabling parallel automation and load distribution.

## Creating Multiple Sessions

Run multiple browser sessions in parallel:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  import concurrent.futures

  client = NotteClient()

  def automate_page(url):
      with client.Session() as session:
          page = session.page
          page.goto(url)
          return page.title()

  urls = [
      "https://example.com/page1",
      "https://example.com/page2",
      "https://example.com/page3"
  ]

  # Run sessions in parallel
  with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
      results = executor.map(automate_page, urls)

  for title in results:
      print(f"Page title: {title}")
  ```
</CodeGroup>

## Session Pool Manager

Create a reusable session pool manager:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  from queue import Queue
  import threading

  class SessionPool:
      def __init__(self, client, size=5):
          self.client = client
          self.size = size
          self.sessions = Queue(maxsize=size)
          self.active_sessions = []

      def initialize(self):
          """Create and start all sessions"""
          for i in range(self.size):
              session = self.client.Session()
              session.start()
              self.sessions.put(session)
              self.active_sessions.append(session)
          print(f"Pool initialized with {self.size} sessions")

      def get_session(self):
          """Get a session from the pool"""
          return self.sessions.get()

      def release_session(self, session):
          """Return a session to the pool"""
          self.sessions.put(session)

      def close_all(self):
          """Close all sessions in the pool"""
          for session in self.active_sessions:
              session.stop()
          print("All sessions closed")

  # Usage
  client = NotteClient()
  pool = SessionPool(client, size=3)
  pool.initialize()

  # Use sessions from pool
  session1 = pool.get_session()
  session1.page.goto("https://example.com")
  pool.release_session(session1)

  # Clean up
  pool.close_all()
  ```
</CodeGroup>

## Parallel Web Scraping

Scrape multiple pages simultaneously:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  from concurrent.futures import ThreadPoolExecutor, as_completed

  client = NotteClient()

  def scrape_product(product_url):
      with client.Session() as session:
          page = session.page
          page.goto(product_url)

          title = page.locator("h1.product-title").inner_text()
          price = page.locator("span.price").inner_text()

          return {"url": product_url, "title": title, "price": price}

  # List of products to scrape
  products = [
      "https://example.com/product/1",
      "https://example.com/product/2",
      "https://example.com/product/3",
      "https://example.com/product/4",
      "https://example.com/product/5"
  ]

  # Scrape in parallel (max 3 concurrent sessions)
  results = []
  with ThreadPoolExecutor(max_workers=3) as executor:
      futures = [executor.submit(scrape_product, url) for url in products]

      for future in as_completed(futures):
          try:
              result = future.result()
              results.append(result)
              print(f"Scraped: {result['title']}")
          except Exception as e:
              print(f"Error: {e}")

  print(f"Total scraped: {len(results)} products")
  ```
</CodeGroup>

## Load Distribution

Distribute work across multiple sessions:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  import threading

  client = NotteClient()

  def worker(session_id, tasks):
      """Worker that processes tasks with its own session"""
      with client.Session() as session:
          page = session.page

          for task in tasks:
              page.goto(task["url"])
              # Process task
              print(f"Session {session_id}: Processed {task['url']}")

  # Split tasks across workers
  all_tasks = [
      {"url": f"https://example.com/page{i}"} for i in range(10)
  ]

  # Divide tasks among 3 workers
  num_workers = 3
  chunk_size = len(all_tasks) // num_workers
  task_chunks = [
      all_tasks[i:i + chunk_size]
      for i in range(0, len(all_tasks), chunk_size)
  ]

  # Run workers in parallel
  threads = []
  for i, chunk in enumerate(task_chunks):
      thread = threading.Thread(target=worker, args=(i, chunk))
      thread.start()
      threads.append(thread)

  # Wait for all workers
  for thread in threads:
      thread.join()

  print("All workers completed")
  ```
</CodeGroup>

## Session Recycling

Reuse sessions for better performance:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  import time

  client = NotteClient()

  def process_batch(session, urls):
      """Process multiple URLs with the same session"""
      page = session.page

      for url in urls:
          page.goto(url)
          # Process page
          print(f"Processed: {url}")
          time.sleep(1)

  # Batch URLs
  batches = [
      ["https://example.com/1", "https://example.com/2"],
      ["https://example.com/3", "https://example.com/4"],
      ["https://example.com/5", "https://example.com/6"]
  ]

  # Process each batch with a dedicated session
  for i, batch in enumerate(batches):
      with client.Session() as session:
          print(f"Session {i} processing {len(batch)} URLs")
          process_batch(session, batch)
  ```
</CodeGroup>

## Proxy Rotation with Pools

Rotate proxies across a pool of sessions:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  from notte_sdk.types import NotteProxy, ProxyGeolocation, ProxyGeolocationCountry
  import concurrent.futures

  client = NotteClient()

  # Define proxies for different regions
  proxies = [
      NotteProxy(geolocation=ProxyGeolocation(country=ProxyGeolocationCountry("US"))),
      NotteProxy(geolocation=ProxyGeolocation(country=ProxyGeolocationCountry("GB"))),
      NotteProxy(geolocation=ProxyGeolocation(country=ProxyGeolocationCountry("DE")))
  ]

  def scrape_with_proxy(proxy, url):
      with client.Session(proxies=[proxy]) as session:
          page = session.page
          page.goto(url)
          return page.title()

  urls = ["https://example.com"] * 3

  # Each session uses a different proxy
  with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
      futures = [
          executor.submit(scrape_with_proxy, proxy, url)
          for proxy, url in zip(proxies, urls)
      ]

      for future in concurrent.futures.as_completed(futures):
          result = future.result()
          print(f"Result: {result}")
  ```
</CodeGroup>

## Rate Limiting

Control the rate of parallel requests:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  import time
  from concurrent.futures import ThreadPoolExecutor

  client = NotteClient()

  class RateLimiter:
      def __init__(self, requests_per_second):
          self.requests_per_second = requests_per_second
          self.min_interval = 1.0 / requests_per_second
          self.last_request = 0

      def wait(self):
          elapsed = time.time() - self.last_request
          if elapsed < self.min_interval:
              time.sleep(self.min_interval - elapsed)
          self.last_request = time.time()

  rate_limiter = RateLimiter(requests_per_second=2)

  def scrape_with_rate_limit(url):
      rate_limiter.wait()
      with client.Session() as session:
          page = session.page
          page.goto(url)
          return page.title()

  urls = [f"https://example.com/page{i}" for i in range(10)]

  with ThreadPoolExecutor(max_workers=3) as executor:
      results = executor.map(scrape_with_rate_limit, urls)

  for result in results:
      print(result)
  ```
</CodeGroup>

## Best Practices

### 1. Limit Concurrent Sessions

Don't create too many sessions at once:

```python
# Good: Reasonable pool size
max_workers = 5

# Bad: Too many concurrent sessions
max_workers = 100  # May overwhelm resources
```

### 2. Use Context Managers

Always use context managers for proper cleanup:

```python
# Good: Automatic cleanup
with client.Session() as session:
    # Work
    pass

# Bad: Manual cleanup (error-prone)
session = client.Session()
session.start()
# Work
session.stop()
```

### 3. Handle Errors

Always handle errors in parallel execution:

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

with ThreadPoolExecutor(max_workers=3) as executor:
    futures = [executor.submit(task, arg) for arg in args]

    for future in as_completed(futures):
        try:
            result = future.result()
            # Process result
        except Exception as e:
            print(f"Task failed: {e}")
            # Handle error
```

### 4. Monitor Resource Usage

Track active sessions and resource consumption:

```python
import psutil

def check_resources():
    cpu = psutil.cpu_percent()
    memory = psutil.virtual_memory().percent
    print(f"CPU: {cpu}%, Memory: {memory}%")

# Check before creating more sessions
check_resources()
```

### 5. Clean Up Properly

Ensure all sessions are closed:

```python
sessions = []

try:
    # Create sessions
    for i in range(5):
        session = client.Session()
        session.start()
        sessions.append(session)

    # Work with sessions

finally:
    # Always clean up
    for session in sessions:
        try:
            session.stop()
        except Exception as e:
            print(f"Error closing session: {e}")
```

## Pool Configuration

Optimize pool settings based on use case:

| Use Case | Pool Size | Strategy |
|----------|-----------|----------|
| **Light scraping** | 3-5 sessions | Sequential batches |
| **Heavy scraping** | 10-20 sessions | Parallel with rate limiting |
| **Testing** | 2-3 sessions | Small pool, rotate proxies |
| **Data processing** | 5-10 sessions | Worker threads with queue |
| **Real-time monitoring** | 1-3 sessions | Long-lived sessions |

## Performance Tips

### 1. Reuse Sessions for Multiple Tasks

```python
# Good: Reuse session for batch
with client.Session() as session:
    for url in urls:
        session.page.goto(url)
        # Process

# Bad: New session per task
for url in urls:
    with client.Session() as session:
        session.page.goto(url)
```

### 2. Use Appropriate Worker Count

```python
import os

# Match CPU cores
max_workers = os.cpu_count()

# Or set based on I/O bound tasks
max_workers = 10  # Good for web scraping
```

### 3. Batch Small Tasks

```python
# Good: Batch URLs
batches = [urls[i:i+10] for i in range(0, len(urls), 10)]

for batch in batches:
    with client.Session() as session:
        for url in batch:
            session.page.goto(url)

# Bad: One session per URL
for url in urls:
    with client.Session() as session:
        session.page.goto(url)
```

## Advanced: Async Session Pool

For async applications:

<CodeGroup>
  ```python Python
  from notte_sdk import NotteClient
  import asyncio

  client = NotteClient()

  async def scrape_async(url):
      # Note: Notte SDK uses sync sessions
      # Run in executor for async compatibility
      loop = asyncio.get_event_loop()

      def sync_scrape():
          with client.Session() as session:
              page = session.page
              page.goto(url)
              return page.title()

      return await loop.run_in_executor(None, sync_scrape)

  async def main():
      urls = [f"https://example.com/page{i}" for i in range(5)]

      # Run concurrently
      results = await asyncio.gather(*[scrape_async(url) for url in urls])

      for result in results:
          print(result)

  asyncio.run(main())
  ```
</CodeGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Session Configuration" icon="gear" href="/features/sessions/configuration">
    Configure individual sessions
  </Card>

  <Card title="Multi-Region Support" icon="globe" href="/features/sessions/multi-region">
    Distribute sessions across regions
  </Card>

  <Card title="Proxies" icon="network-wired" href="/features/sessions/proxies">
    Rotate proxies in session pools
  </Card>

  <Card title="Session Lifecycle" icon="clock" href="/features/sessions/lifecycle">
    Manage session state
  </Card>
</CardGroup>
