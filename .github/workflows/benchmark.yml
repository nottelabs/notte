name: benchmark

on:
  workflow_dispatch:
    inputs:
      agent_model:
        description: "Model agent with which to run the benchmarks"
        required: true
        default: "cerebras/llama-3.3-70b"

      n_jobs:
        description: "Number of parallel jobs to run"
        required: false
        default: "2"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  CACHE_TYPE: "pip"

jobs:
  run-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4


      - name: Set environment variable
        run: echo "CEREBRAS_API_KEY_CICD=${{ secrets.CEREBRAS_API_KEY_CICD }}" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: ${{ env.CACHE_TYPE }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit pytest mypy pytest-asyncio pytest-mock
          pip install -e .
          pip install types-requests types-beautifulsoup4 types-regex types-chevron pandas tabulate joblib
          patchright install --with-deps chromium

      - name: Run benchmark unit tests
        run: |
          pytest tests -k "test_benchmark_webvoyager"  --ignore=tests/integration/test_resolution.py  --ignore=tests/integration/test_webvoyager_resolution.py --ignore=tests/browser/test_pool.py --agent_llm ${{ github.event.inputs.agent_model }} --n_jobs ${{ github.event.inputs.n_jobs }} --show-capture=log

      - name: Upload md results as step summary
        if: always()
        run: cat dist/results.md >> $GITHUB_STEP_SUMMARY

      - name: Upload json results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: JSON results
          path: dist/results.jsonl
