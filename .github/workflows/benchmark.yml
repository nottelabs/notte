name: benchmark

on:
  push:
  workflow_dispatch:
    inputs:
      agent_model:
        description: "Model agent with which to run the benchmarks"
        required: true
        default: "cerebras/llama-3.3-70b"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  CACHE_TYPE: "pip"

jobs:
  tests:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: ${{ env.CACHE_TYPE }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit pytest mypy pytest-asyncio pytest-mock
          pip install -e .
          pip install types-requests types-beautifulsoup4 types-regex types-chevron
          patchright install --with-deps chromium

      - name: Run benchmark unit tests
        run: |
          pytest tests -k "test_benchmark_webvoyager"  --ignore=tests/integration/test_resolution.py  --ignore=tests/integration/test_webvoyager_resolution.py --ignore=tests/browser/test_pool.py --agent_llm ${{ github.event.inputs.agent_model }}  -s
