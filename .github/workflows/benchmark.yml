name: benchmark

on:
  workflow_dispatch:
    inputs:
      config:
        description: "Full toml config"
        required: true
        default: |
          [RunParameters]
          n_jobs = 3
          tries_per_task = 1
          evaluator = "None"
          capture_logging = true

          [RunParameters.task_set]
          name = "WebVoyagerSimple"

          [Falco]
          use_vision = false
          headless = true
          model = "cerebras/llama-3.3-70b"
          max_steps = 20
          history_type = "short_observations_with_short_data"
          pool = "None"

concurrency:
  group: >-
    ${{ github.workflow }}-${{ github.ref }}-
    ${{ github.event.inputs.agent_model }}-
    ${{ github.event.inputs.n_jobs }}-
    ${{ github.event.inputs.include_screenshots }}-
    ${{ github.event.inputs.history_type }}-
    ${{ github.event.inputs.tries_per_task }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  CACHE_TYPE: "pip"

jobs:
  run-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4

      - name: Set environment variables
        run: |
          echo "CEREBRAS_API_KEY_CICD=${{ secrets.CEREBRAS_API_KEY_CICD }}" >> $GITHUB_ENV
          echo "OPENAI_API_KEY_CICD=${{ secrets.OPENAI_API_KEY_CICD }}" >> $GITHUB_ENV

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: ${{ env.CACHE_TYPE }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pre-commit pytest mypy pytest-asyncio pytest-mock
          pip install -e .
          pip install types-requests types-beautifulsoup4 types-regex types-chevron pandas tabulate cloudpickle
          patchright install --with-deps chromium

      - name: Run benchmark unit tests
        run: |
          pytest tests/integration/test_e2e.py \
                 --capture=no \
                 -p no:asyncio \
                 --config ${{ github.event.inputs.agent_model }}

      - name: Upload md results as step summary
        if: always()
        run: cat dist/results.html >> $GITHUB_STEP_SUMMARY

      - name: Upload Logs / Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            dist/*
